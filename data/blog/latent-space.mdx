---
title: Learning Latent Embeddings 
date: '2024-07-14'
tags: ['math', 'machine learning', 'representation learning']
draft: false
summary: 'An overview of commonly used techniques associated with latent embeddings'
authors: ['default']
images: ['/static/images/latent-images.jpg', '/static/images/latent-images.jpg', '/static/images/moco.png', '/static/images/transformer-latent.png']
---

### **Contrastive Learning** 

Contrastive Learning is the most commonly used technique to train large embedding models with transformers.

The goal of contrastive learning is to learn dense vector representations of the input modality (e.g. images, text, video) where distances between similar items are close and dissimilar items are far apart. 


**Siamese Networks**

First introduced in [Bromsky et al.](https://papers.neurips.cc/paper_files/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf) as a way to perform signature verification,
the Siamese Network is a neural network architecture that makes use of two towers or encoders to learn representations between similar and dissimilar items. 

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
  <img src="/static/images/siamese.png" style={{width: '80%'}}/>
  <figcaption>Siamese Network</figcaption> 
</figure>

It does this by minimizing the objective $||f(x^i) - f(x^j)||^2$ when the input images $x$ are the same and maximizing the objective when they 
are different. 

**Contrastive Loss**

We can formulate a loss objective where we define the similarity as the dot product of the L2-normalized feature representations. 

<div class="centered text-xl">
$$ sim(x, y) = \frac{f(x)}{|| f(x)||_2} \cdot \frac{f(y)}{||f(y)||_2} $$
</div>

Given that x is our query or $x^+$ and $x^-$ is our in-batch positives and negatives. 

Our loss is defined as:

<div class="text-xl">
$l(x, x^+) = - \log \frac{\exp (sim(x, x^+ ) / \tau)}{ \exp(sim(x, x^+)/\tau) + \sum^N_{j=1} \exp(sim(x, x_j^-)/ \tau)} $
</div>


One of the first papers to explore this idea of contrastive learning for image embeddings was Momentum Encoder or MoCo ([He et al. 2019](https://arxiv.org/pdf/1911.05722))

The key takeaway from MoCo was that it makes use of a dummy encoder from which resolves the computational cost of training a unique encoder without losing quality of negative encodings over time. 

<div style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
$$w_k \leftarrow m w_k + (1 - m) w_q$$
</div>

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
  <img src="/static/images/moco.png" />
  <figcaption>Momentum Encoder</figcaption>
</figure>

**Triplet Loss** 

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
  <img src="/static/images/triplet.png" />
  <figcaption>Learning triplets</figcaption>
</figure>

Triplet loss can be viewed as a better contrastive loss. 
Given an anchor $A$, a positive sample $P$, and a negative sample $N$, Triplet seeks to minimize the objective 

$$l(A,P,N) = \max((|f(A)-f(P)|)^2 - (|f(N)-f(A)|)^2 + \alpha, 0)$$

where each absolute value term represents the distance between the encodings learned of anchor A and the negative and positive samples.

We want to make sure $(|f(A)-f(P)|)^2 - (|f(N)-f(A)|)^2$ or the distance between the anchor and positive term is less than the distance between 
the anchor and the negative term. Additionally, sometimes we have threshhold $\alpha$ to ensure that the model learns that positives and anchors are much more similar 
than negatives and anchors.


### **Relationship with Language Modeling**

Computing a vector similarity between latent vectors to extract meaning behind the data is reminiscent to the core principle behind self-attention in language models such as GPT (Generative Pre-Trained Transformer)

From the diagram below, a transformer learns through self-attention the similarity level between different words or tokens in the input dictionary. 

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
  <img src="/static/images/transformer-latent.png" style={{width: '80%', height: '80%'}}/>
  <figcaption>Transformer as an Inquiry System</figcaption>

</figure>

When we generate new words, the model selects the word with the highest probability in relation to the query and keys computed by a weighted similarity or attention function $softmax(\frac{QK^T}{\sqrt{d_k}})V$. 

More specifically, this computation is represented by a lookup table defined by learnable parameters from our word embedding $x_i$ and our encoder networks $Q$, $K$, and $V$.

$$ q_i = Qx_i \text{ (queries) }  \quad   k_i = Kx_i \text{ (keys)     }  \quad v_i = Vx_i \text{ (values)}$$

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
  <img src="/static/images/lookup_table.png" style={{width: '50%', height: '50%'}} />
  <figcaption>Lookup Table for Queries</figcaption>
</figure>


**High-level overview**

At a high level, a transformer is learning an implicit probability transition matrix based off of predicting the next 
term from the context. This can be formalized in multi-headed attention like so:

$$p(x_t | x_{t-1} ... x_{1}) = softmax(FFN_n(max(0, (Norm(Concat(h_1, h_2, ... , h_k) W^0 )))))$$

where an attention head $h_i$ is computing the function 

$$h_i = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

with a multi-layer feed forward network defined as:

$$FFN_i(x) = max(0, W_{i-1}(FFN_{i-1}(x) + b))W_i + b$$


### Dimensionality Reduction 

Understanding the nature of the high-dimensional representations your model is learning can seem overly abstract especially when working with vectors
of size 1000 and greater. 

Fortunately, there exist dimensionality reduction techniques to visualize and understand these embeddings.
One commonly used technique for dimensionality reduction of embeddings is UMAP (Uniform Manifold Approximation and Projection).

The core idea of UMAP is to find a global topological structure of the data. More on this [here](https://www.youtube.com/watch?v=nq6iPZVUxZU).

Below is an example of how UMAP can represent text embeddings of different internet content you might see in your "For You Page" in 3D.


<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  width: '100%'
}}>
  <iframe 
    src="/visual.html" 
    style={{
      width: '90%',
      height: '500px'
    }}
  />
  <figcaption>Interactive graph of content embeddings with UMAP (Use arrow keys to explore)</figcaption>
</figure>


### Conclusion 

Thanks for reading up to this point!

Hopefully, you learned a little about representation learning and gained some intuition on how 
DL models learn semantic relationships through encoding latents. 

### Acknowledgements
Thank you to [Andrew Zhang](https://x.com/Andrew_Zhang0) for feedback on earlier drafts!

{/*
### **Variational Autoencoders**

Diffusion models originated from variational auto-encoders. They they are trained on an objective based off of sampling a latent $z \sim \mathcal{N}(0,1)$ from gaussian noise.

A VAE defines an intractable density function where $x$ is our prior distribution:

<div class="text-xl">
    $$p_{\theta}(x) = \int p_{\theta}(z) p_{\theta}(x | z) dz$$
</div>
**Conditional Variational Autoencoder** uses Encoder $q_\phi (z | x)$ and Decoder $p_\theta(x|z)$ networks to learn 
a latent representation of the input data with respect to the output data. 

This seems intuitive because people reconstruct images in their minds all the time even if they don't have perfect memory of the 
exact details. 

 - [CLIP Paper](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf)
*/}
### Resources 

 - [C4W4L03 Siamese Network](https://www.youtube.com/watch?v=6jfw8MuKwpI)
 - [Momentum Encoder](https://arxiv.org/pdf/1911.05722)
 - [C4W4L04 Triplet loss](https://www.youtube.com/watch?v=d2XB5-tuCWU)
 - [CS480/680 Lecture 19: Attention and Transformer Networks](https://www.youtube.com/watch?v=OyFJWRnt_AY)
 - [Stanford CS224N NLP with Deep Learning | 2023 | Lecture 8 - Self-Attention and Transformers](https://www.youtube.com/watch?v=LWMzyfvuehA)
 - [What exactly are the Keys, Values, and Queries in a Transformer?](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms/531971#531971)